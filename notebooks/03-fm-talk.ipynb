{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to neural networks with Keras\n",
    "Florent Martin (Regensburg Universit√§t)  \n",
    "March 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks form one of the models used in machine learning.\n",
    "What is machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.rc('axes',titlesize='xx-large')\n",
    "plt.rc('axes',labelsize='x-large')\n",
    "plt.rc('legend',fontsize='x-large')\n",
    "%matplotlib inline\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# (Section 1) Before neural networks: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 1.1 The iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "iris = sns.load_dataset(\"iris\")\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(iris, hue=\"species\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: knowing the petal width, predict if the iris is a virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['isVirginica'] = (iris['species'] == 'virginica').apply(int)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "iris.groupby('isVirginica').hist(column='petal_width', ax=ax, bins=15);\n",
    "plt.legend(['not virginca', 'virginica']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "The **logistic regression** algorithm searches for a function \n",
    "$$ P: [0,3] \\to [0,1]$$\n",
    "If $x = $ petal width, $P(x)$ is an estimate (= a guess) of the probability that the plant is a virginica.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "The idea of logistic regression is to look for a function of the form:\n",
    "$$P_{w,b}(x) = \\sigma(Wx+B)$$\n",
    "where \n",
    "$$\\sigma : \\mathbb{R} \\to [0,1]$$ is the **sigmoid function ** defined by \n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "We call $W$ the **weight** and $B$  the **bias**.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-10, 10, .01)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.title('The sigmoid function');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 7))\n",
    "ax1.set_title('Effect of B value in sigmoid')\n",
    "for b in [-5, -2, 0, 2, 5]:\n",
    "    ax1.plot(x, sigmoid(1 * x + b),label='B={}'.format(b))\n",
    "    ax1.legend()\n",
    "\n",
    "ax2.set_title('Effect of W value in sigmoid')\n",
    "#for w in [0.05, .15, .3, .5, 1, 3]:\n",
    "for w in [-1,0.05, .3, 3]:\n",
    "    ax2.plot(x, sigmoid(w * x + 0),label='W={}'.format(w))\n",
    "    ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(iris[['petal_width']], iris[['isVirginica']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_widths = np.arange(0,3,0.01)\n",
    "predicted_proba = model.predict_proba(petal_widths.reshape(-1,1))[:,1]\n",
    "plt.plot(petal_widths, predicted_proba, 'r--')\n",
    "plt.xlabel('petal_width')\n",
    "plt.ylabel('predicted probability');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "iris.groupby('isVirginica').hist(column='petal_width', normed=True, ax=ax);\n",
    "plt.legend(['not virginca', 'virginica'])\n",
    "plt.plot(petal_widths, predicted_proba,'r--',label='predicted proba')\n",
    "plt.hlines(0.5, *ax.get_xlim(), linestyles='dotted')\n",
    "plt.vlines(petal_widths[np.argmax(predicted_proba > 0.5)], *ax.get_ylim(), linestyles='dotted');\n",
    "petal_widths[np.argmax(predicted_proba > 0.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$ \\text{Accuracy} = \\frac{\\text{number of samples correctly classified}}{\\text{total number of samples}}$$\n",
    "<size>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with sklearn the method score returns the accuracy\n",
    "model.score(iris[['petal_width']], iris[['isVirginica']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical representation of logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logistic regression](../reports/figures/01-log.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers, optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils.vis_utils import plot_model, model_to_dot\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(1, input_dim=1),\n",
    "    Activation('sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', \n",
    "                    metrics=['accuracy'],\n",
    "                    loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(iris[['petal_width']], iris[['isVirginica']], epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(iris[['petal_width']], iris[['isVirginica']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "iris.groupby('isVirginica').hist(column='petal_width', normed=True, ax=ax);\n",
    "plt.legend(['not virginica', 'virginica'])\n",
    "predicted_proba = model.predict(petal_widths.reshape(-1,1))[:,0]\n",
    "plt.plot(petal_widths, predicted_proba,'r--',label='proba')\n",
    "plt.hlines(0.5, *ax.get_xlim(), linestyles='dotted')\n",
    "plt.vlines(petal_widths[np.argmax(predicted_proba > 0.5)], *ax.get_ylim(), linestyles='dotted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Section 2) Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Optimization, gradient descent with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(1,input_dim=1,kernel_regularizer=regularizers.l2(.1)),\n",
    "    Activation('sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.SGD(lr=0.05), \n",
    "              loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid, y_grid = np.mgrid[-10:10:.1,-10:10:.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(weight,bias):\n",
    "    layer =  model.layers[0]\n",
    "    layer.set_weights( [ np.array([[weight]]) , np.array([bias]) ] ) \n",
    "    accuracy = model.evaluate(iris[['petal_width']], iris[['isVirginica']],verbose=0)[1]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccuracy = np.vectorize(get_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes two minutes to run\n",
    "acc_grid = vaccuracy(x_grid,y_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('../data/acc_grid', 'wb') as f:\n",
    "#     # Pickle the 'data' dictionary using the highest protocol available.\n",
    "#     pickle.dump(acc_grid, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/acc_grid', 'rb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    acc_grid =pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax_acc = plt.subplots(figsize=(15,7))\n",
    "plt.pcolor(x_grid , y_grid , acc_grid , cmap='RdBu_r')\n",
    "plt.colorbar()\n",
    "plt.title('Accuracy',fontsize='xx-large')\n",
    "plt.xlabel('Weight (w)',fontsize='x-large')\n",
    "plt.ylabel('Bias (b)',fontsize='x-large')\n",
    "ax_acc.tick_params(labelsize='large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: the accuracy is constant on huge zones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 The cross entropy  loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "Fix the weigth $w$ and bias $b$. \n",
    "Define \n",
    "<br><br><br>\n",
    "$$\\mathcal{L}(p,y) = y \\log(p) + (1-y)\\log(1-p)$$\n",
    " <br>\n",
    "$$\\mathcal{L_{w,b}} = \\sum_{i=1}^n y_i \\log(p_i) + (1-y_i)\\log(1-p_i)$$\n",
    "<br>\n",
    "where </font>\n",
    "\n",
    "*  <font size=4>$y_i\\in \\{0,1\\}$ is the actual class of the i-th sample</font>\n",
    "\n",
    "*  <font size=4>$p_i \\in [0,1]$ is the predicted probability $P_{w,b}(x_i)$ calculated by the logistic regression model for parameter values $w$ and $b$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "probas = np.arange(0,1,.01)\n",
    "loss0 = -np.log(1-pp)\n",
    "loss1 = - np.log(pp)\n",
    "plt.plot(probas,loss0,label='y = 0')\n",
    "plt.plot(probas,loss1,label='y = 1')\n",
    "plt.xlabel('proba')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss function:  $\\mathcal{L}(p,y)$')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_keras_model(initial_weight = 9,initial_bias = 9,lr=0.05,reg=.1):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1,input_dim=1,kernel_regularizer=regularizers.l2(.1)))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    sgd = optimizers.SGD(lr=lr)\n",
    "    model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    layer =  model.layers[0]\n",
    "    layer.set_weights( [ np.array([[initial_weight]]) , np.array([initial_bias]) ] ) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(weight,bias):\n",
    "    layer =  model.layers[0]\n",
    "    layer.set_weights( [ np.array([[weight]]) , np.array([bias]) ] ) \n",
    "    loss = model.evaluate(iris[['petal_width']], iris[['isVirginica']],verbose=0)[0]\n",
    "    return loss\n",
    "\n",
    "vloss = np.vectorize(get_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time loss_grid = vloss(x_grid,y_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('../data/loss_grid', 'wb') as f:\n",
    "#     # Pickle the 'data' dictionary using the highest protocol available.\n",
    "#     pickle.dump(loss_grid, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/loss_grid', 'rb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    loss_grid =pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig , ax_loss = plt.subplots(figsize=(20,7))\n",
    "plt.pcolor(x_grid , y_grid , loss_grid , norm=colors.LogNorm() , cmap='RdBu_r')\n",
    "plt.colorbar()\n",
    "contour = plt.contour(x_grid, y_grid, loss_grid,20)\n",
    "plt.title('Loss function')\n",
    "plt.xlabel('weight')\n",
    "plt.ylabel('bias')\n",
    "model.layers[0].set_weights( [ np.array([[9]]) , np.array([9]) ] ) \n",
    "for i in range(30):\n",
    "    old_weight, old_bias = model.get_weights()[0][0][0], model.get_weights()[1][0]\n",
    "    model.fit(iris[['petal_width']], iris[['isVirginica']], epochs=5, verbose=0)\n",
    "    weight, bias = model.get_weights()[0][0][0], model.get_weights()[1][0]\n",
    "    plt.plot([old_weight,weight],[old_bias,bias],'kX--',markersize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Section 3) Neural Networks: when linear methods are not sufficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Versicolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['isVersicolor'] = (iris['species'] == 'versicolor').apply(int)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "iris.groupby('isVersicolor').hist(column='petal_width', ax=ax, bins=15);\n",
    "plt.legend(['not versicolor', 'versicolor']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: determine if an iris is a versicolor knowing its petal width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Let's try  logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(1,input_dim=1,kernel_regularizer=regularizers.l2(.1)),\n",
    "    Activation('sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.SGD(lr=0.05), \n",
    "              loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(iris[['petal_width']], iris[['isVersicolor']], epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "iris.groupby('isVersicolor').hist(column='petal_width', normed=True, ax=ax);\n",
    "plt.legend(['not versicolor', 'versicolor'])\n",
    "predicted_proba = model.predict(petal_widths.reshape(-1,1))[:,0]\n",
    "plt.plot(petal_widths, predicted_proba,'r--',label='proba')\n",
    "plt.hlines(0.5, *ax.get_xlim(), linestyles='dotted')\n",
    "plt.vlines(petal_widths[np.argmax(predicted_proba > 0.5)], *ax.get_ylim(), linestyles='dotted');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAD NEWS: PREDICTED PROBAS BY LOGISTIC REGRESSION CAN NOT UP AND DOWN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Add complexity to the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(3, input_dim=1 ) )\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1, input_dim=1 ) )\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(optimizer=optimizers.SGD(lr=.1), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NN](../reports/figures/02-hidden.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(nrows=3,ncols=3,figsize=(18,12),sharex=True,sharey=True)\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3, i)\n",
    "    model.fit(iris[['petal_width']], iris[['isVersicolor']],epochs=150,verbose=0)\n",
    "    probas = model.predict(petal_widths.reshape(-1,1))[:,0]\n",
    "    plt.plot(petal_widths,probas,label='{} epochs'.format(i*150))\n",
    "    plt.ylim((0,1))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "iris.groupby('isVersicolor').hist(column='petal_width', normed=True, ax=ax);\n",
    "plt.legend(['not versicolor', 'versicolor'])\n",
    "predicted_proba = model.predict(petal_widths.reshape(-1,1))[:,0]\n",
    "plt.plot(petal_widths, predicted_proba,'r--',label='proba')\n",
    "plt.hlines(0.5, *ax.get_xlim(), linestyles='dotted')\n",
    "plt.vlines(petal_widths[np.argmax(predicted_proba > 0.5)], *ax.get_ylim(), linestyles='dotted');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
