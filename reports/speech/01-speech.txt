### DON'T FORGET TO RUN THE FIRST CELL OF THE NOTEBOOK ###

Hello everyone
First I would like to thank the orgaizers of PyData Munich and super Python talks and in particular 
Nick del Grosso who got much involved in the last weeks and  help me to improve the quality of this presentation.

Before starting I would like to encourage all of you 
to interupt me and ask a question something is not clear during this talk,
or if you have a remark.

So the goal of the 45 minutes is to introduce neural networks and see how to use them with the Python library Keras.

First an overall introduction: 

What is machine learning? 
It is a field of computer where one solves a given task, and instaed of 
developping a classical algorithm to solve this task, one  chooses a model. 
And the important thing is that the model   depends on some  parameters 
(in most cases those parameters are numbers).
And to use the model, 
you need to fix the parameters of the model.
To do that, the strategy of machine learning is to use some data 
(often this means: instances of the problem with their solution)
and after seeing all the data, your model
chooses the best parameters that fit the data.

Now, neural networks are just a family ofmodels which are used in machine learning.
And I want to say that neural networks are just one example of models for machine learning 
but there are many others models 
(for instance Decision trees, support vector machines, naive Bayes classifiers and so on).

And Keras is a Python library which allows to use neural networks.

So the goal of this presentation is to solve a  simple problem (flower classification) 
using Keras and try to understand how this works.

Before starting I want to anticipate a make a sketchy overall of what we'll see.
Here on the top right you see a graphical representation of a neural network.
The idea is that you've an input (which is a number) and you want an output (which is also a number) and to make the computation of the output there are some intermediate nodes. 
One calls this layers in the middle some hidden layers (in the sense that they are not part of the input 
nor of the output) but still are crucial in the computation.

And I should say that the parameters are used in the intermediate nodes.
This means that each node in this computational graph depends 
on some parameters. 
On the left side, what you see is a neural network which is simpler and this very kind of neural network have there own theory, and their own names: namely they are called logistic regression classifiers.
So actually in this talk, we'll first investigate logistic regression (because it's 
simpler to understand) and then we'll move to more complicated neural networks like on the right.
And the curve you see on the left, is the typical kind of curve a logistic regression 
classifier can learn. 
And the curve on the right as we'll see t is a typical example of a curve that  can't be learnt from a logistic regression classifier but can be learnt by a general neural network like this one.

The plan will be as follow:
there will be 3 parts. 
The first part will be about logistic regression.
First we'll investigate a problem of flower classificatin, for that we'll use the socalled iris dataset.
Then we'll  implement our first logistic regression using scikit-learn which is 
a very general  library to do machine learning with python.
And we'll finally use Keras to implement a logistic regression.

In the second part we'll talk about gradient descent which is 
the technique that neural networks use to fit the parmaeters of the model with the data.
For that we'll talk about optimization, and then about cross entropy loss function.

And in the third and last part, we'll implement with Keras a neural network with 
a hidden layer like we saw on the previous slide.
For that we'll change a little bit the problem to solve, then 
try to use logistic regression, but it won't work we'll try to understand why and what are the limitations of logistic regression, 
and finally we'll be happy to see that neural network with a hidden can solve our problem.

Before starting, I will import the python libraries we will need.
Numpy which is used to manipulate arrays of numbers 
and do some numerical computations, tehn
Matplotlib which is a library to plot 2-dimensional graphs, then 
pandas which allows to manipulate  datasets
and finally seaborn which is a vizualisation library which we use on top of matplotlib and 
allows to improve the quality of the plots.

Let's now start the first part about logistic regression.
As you'll see, logistic regression is an example among 
many others of neural networks, and one could say that 
logistic regression is the simplest neural network one can build.
And I believe that before seeing the general notion of neural networks, 
it is better to get a first contact with logistic regression. 


For that we'll ivestigate a  flower classification problem and use  
a famous data set called the iris dataset.

It can be loaded directly with seaborn using the fonction load_dataset.
Now the variable iris is a dataframe and we can  see 5 random rows of it 
using the method sample of the dataframe iris.

Each row in the dataframe is a sample of an iris.
We have 5 columns : sepal length, sepal width, petal length and petal width and 
a last colums that gives the species of the sample 
corresponding to the row.
We can see the number of rows and colums  of the datframe using its method shape: 
it has 150 rows (which means for us 150 samples) 
and 5 columns (which we had already seen above). 
And we can see how species are distributed in this dataset usnig the 
method value_counts on the column species, 
and we see that there are in total 3 species : setosa, versicolor and virginic and that we have 50 samples of each.

In case you wonder what an iris is, or what a setoss, a versicolor or a virginica is, here are pictures of
each of these species.

With seaborn we can draw a pairplot of
the dataset, 
where in blue you have setosa, in green versicolor and in red virginica. 
On the diagonal, we have histograms of the 4 properties 
(petal widt, petal length, and so on) and 
out of the diagonal, we have scatter plots between two 
of the features (for instance a scatter plot of the petal length verus petal width).

Problem we'll try to solve is the following: 
if a give you the petal width of a sample, 
I want to know if the sample is of the species virginica.
In other words, the  input is  the petal width of 
an iris sample.
The question is: is the sample's species virginica or not?
Hence, the ouput an answer to the question: is the plant a virginica?
The anser could be yes (or 1 if we want a number representation) or the answer 
could be False (or equivalently 0).

To work on this we'll ad a column called 'is_virginicia' to our dataset, and the value of the column will be 
0 if the plant is not a virginica and 1 if it is  a virginca.
And we can call the method sample and see 5 random samples and
see the new column 'is_virginica'.

To answer that question, it might be good to plot a histogram of the petal width
To do that we can use the method hist (for histogram) of a groupby objecti.
To make the histogram nice,  we add a legend which 
will plot the bins of the histogram with different colors
for the species depending on being of the species virginica or not, 
and we label the x-axis with the 'petal width', 
and we label the y-axis number of sample.
The histogram we get is the petal width on the x-axis,
, on the y-axis, how many samples in our dataset have this petalwidth, and in green we have virginica samples while in blue we have non virginica.

What's clear from this histogram is that on the right side, 
we have green bins, hence
the species of the samples is virginica, 
while on the left side, we have blue bins, 
samples are not virginica
More precisely, if we want to answer our problem, 
namely answering the question 
'is the iris sample of the species virginica, for values lower than 
1.3, the answer is False/no/0,
and when the petal width is greater than 1.8 the answer is yes it is virginica,
so the output is True or equivalently 1.

So just from this diagram, we can start trying to solve our problem, and we'll draw our answer using a dotted 
red curve.
From the analysis we already made 
for petal width smaller than 1.3 the anser is 0 
(keep in mind that 0 = False)
and for petal width values greater than 1.7, 
the anser is True which we numerically represent with 1).

But in between it's not so clear.
For instance when the petal width is 1.5 there are as many samples which are virginica than not virginica.
So I would say that there is no obvious answer for such values.
So one natural possibility here would be to draw a line to join these two partial dotted red curves we already have.
We would get that for instance. 

That looks like a reasonable guess, but could also have tried to make something smoother like that: 
with this solution to the problem,  the 
output is again clearly 0 so false on left, and 1 so true on the right but the difference is that the transition is very smooth.

What we did so far was trying to solve the problem using our intuition, so now let's do this using Logistic regression has I promised it earlier.

For the problem we are conidering, the output of the logistic regression classifier will be a function having the petal width as input and returning as output a probability. 
And I want to stress here that a probability is a number contained between 0 and 1 and the closer it is to 0, the more the prediction is close to False, and the closer it is to 1, the closer the prediction is to be True.
Ultimaltely, the probability is used to anser the problem by saying: 
if the probability is greater then 0.5, then return True, if the probability is less than 0.5, return False.

To model probabilities the key ingredient of logistic regression is the sigmoid function.
It is the function that takes as input x and returns 1 divided by 1 + exponential of -x
This might be a little bit obscure, but let's try to plot the sigmoid function.
I define a Pythonfunction the computes the sigmoid functiona and guess how we call it: sigmoid
And we plot it

Before when we've been trying to guess dotted red curve for the probability of being virginica, we got similar curves.
Let me highlight some key properties of the curve of the sigmoid function. 
It takes values between 0 and 1 (that's we want because we want to use it to return a probability and as we said before a probability should be between 0 and 1.
It reaches the value 0.5 (one half exactly for the value x=0) and the more you go on the right, the more you tend, and the more you go on the left, the more you tend to 0.

So the idea of logistic regression is to use the sigmoid function to output the probability knowing the value of the petal width.
So let's replot the histogram of the petal widths and plot the sigmoid on the same graph.

Hum, what did we expect?  The sigmoid curve is the same as before and namely it predicts high probability (higher than 0.5) for positive values. 
So if we were using the sigmoid red curve to predict the probabilities of being virginica, we would anser True for all positive values, and since of course, the petal widths are always positive values, we would always answer True. which is bad since there are lots of plants which are not virginica.

That's where the key idea to introduce paramters enters in the game.
Namely we are going to modify the sigmoid function using two parameters
a weigth W i first give it the value 1
and a bias B i first give it the value 0
and we modify the function to compute the probability replacing x by w*x + b
If I plot again, I get the same thing, because w =1 b = 0 hence w times x + b 
equals 1*x + 0 equals x

But let's change w and make it 5 for instance. 
Can someone tell me what's going happen?

the curve is compressed around x=0
in particular the zone where the probabilities change from 0 to 1 is much smaller
this probability curve is giving which confident predicitons.

I put the value w = 1 and b= 0 again

Now what happen if I set w to 3? Has someone an idea?
it shiffts the curve to the left by 3 units
to be sure, i set w to -3 it should shift the curve to the right
yes
let's set w to 1 b =0 to get the genuine sigmoid function
and let's try to make it feet with the data we have: namely the output should be yes 
(close to 1) for values higher than 2
and close to 0 for values lower than 1.5
set w=3 to get a curve we a important slope
and now we want to shift the curve to the right, so we need a negative value for b.
try b = -1 , not enough
try b = -10 too much
try b= -5 OK
To sum up with logistic regression:
it is a model which depends on 2 parameters W called the weight and B the bias and
it ouputs a probability 
sigmoid(Wx + b)

So fine but in practice, you want your computer to do the job for you and guess himself the parameters.

For that we're gonna use first the python library scikit learn which is a library that implements many machine learning models.

We import the class Logistic regression from scikit learn
and we instatiate an instance of a logistic regression classifier and sve it to the variable model

But at that point, the model has not seen any data.
And the philosophy from machine learning is that the model should adapt to the 
data we have, and it should learn and modify its parameters so that they fit  the data
To do that, you have to call the fit method from the model, 
and give him the input data and the output in our case, the ouutput 
is 0if th plant is not virginica and 1 if the plant is virginica.

Now the model has read the data, and hopefully now the parameters fit the data

We can use the attributes coefwhich represents the weight and interept which is the bias

And it is quite close to what we had found ourselves before: we had by hand playing 
with the curve guessed w=3 b =-5

So let's try to plot the probability curve
for that we use the method predict_proba from our model
and we've got this curve,
this really agrees with what we've seen so far, it looks like the sigmiod graph 
shifted to the right

Let's plot on top of it the histogram we had
And indeed the predicted probabilities from our logistic regression model ]
which are plotted in red are very satisfying (at least very close 
to what we had suggested before)

Now an important question is how to know if your model is doing going 
on the data?
Here of course it is easy because the input has only one coordinate, 
but if there were many inputs it could be difficult to evaluate the model.

For that one introduces the accuracy of the model : 
it is the ratio between number of samples in the data which have 
been correctly classified and the total number of samples

with scikit-learn, the method score from the model returns the accuracy
and you need to give as input to this method the 
input values of the daatset (in our case the petal eidths) 
and the outuput (1 if the plant is virginica, 0 otherwise)




