Hello everyone
First I would like to thank the orgaizers of PyData Munich and super Python talks and in particular Nick del Grosso who spent a lot of time and energy hhelping me improve the quality of this presentation.

Before starting I want to say, please, if something is not clear during this talk, please ask me a question and I'll be happy to try to answer it.

I want to introduce you to neural networks and see how to use them with the Python library Keras.

First an overall introduction: 

What is machine learning? 
It is a field of computer where one solves a specific task, and instaed of 
developping a classical algorithm to solve this task, one instead chooses a model. 
And the important thing is that the model you choose depends on some  parmaters 
(in most cases those parameters are numbers).
And to use the model, 
you need to fix the parameters of the model.
To do that, the strategy of machine learning is to use some data 
(often this means: instances of the problem with their solution)
and after seeing all the data, your model
chooses the best parameters that fit the data.

Now, neural networks are models which are used in machine learning.
And I want to say that neural networks are just one type of models for machine learning 
but there are many others 
(for instance Decision trees, support vector machines, naive Bayes classifiers and so on).

And Keras is a Python library which allows to use neural networks.

So the goal of this presentation is to takle simple problem (flower classification) 
using Keras and try to understand how this works.

Before starting and want to anticipate a make a sketchy overall of what we'll see.
Here on the top right you see a graphical representation of a neural network.
The idea is that you've an input (which is a number) and you want an output (which is also a number) and to make the computation of the output there are some intermediate nodes. 
One calls this layers in the middle some hidden layers (in the sense that they are not part of the input 
nor of the output) but still play a crucial in the computation.

And I should say that the parameters are used in the intermediate nodes.
On the left side, what you see is a neural network which is simplerm and this very kind of neural network have there own theory, and their own names: namely they are called logistic regression classifiers.
So actually in this talk, we'll first investigate logistic regression (because it's 
simpler to understand) and then we'll move to more complicated neural networks like on the right.
And the curve you see on the left, is the typical kind of curve a logistic regression 
classifier can learn. 
And the curve on the right as we'll see on the right is a typical example of a curve that  can't be learnt from a logistic regression classifier but can be learnt by a general neural network like this one.

The plan will be as follow:
there will be 3 parts. 
The first part will be about logistic regression.
First we'll investigate a prblem of flower classificatin, for that we'll use the socalled iris dataset.
Then we'll first implement our first logistic regression using scikit-learn which is 
a general machine learning library for python.
and we'll finally use Keras to implement a logistic regression

In the second part we'll talk about gradient descent which is the technique that neural networks use to fit the parmaeters of the model to the data.
For that we'll talk about optimization, and then about cross entropy loss function.

And in the third and last part, we'll implement with Keras a neural network with 
a hidden layer like we saw on the previous slide.
For that we'll change a little bit the problem to solve, then 
try to use logistic regression, but it won't work we'll try to understand why and what are the limitations of logisstic regression, 
and finally we'll be happy to see that neural network with a hidden can solve our problem.

Before starting, I will import python libraries we will need.
Numpy which is used to manipulate arrays of numbers 
Matplotlib which is a library to plot 2-dimensional graphs
pandas which allows to manipulate  datasets
and seaborn which is a vizualisation library which we use on top of matplotlib and 
allows to improve the quality of the plots.

It's time to start the first part about logistic regression.
For that we'll ivestigate a  flower classification problem and use for that 
a famous data set called the iris dataset.

It can be loaded directly with seaborn using the fonction load_dataset.
Now the variable iris is a dataframe and let's see 5 random rows of it.
For that I use the method sample from the dataframe iris and ask to see 5 random rows

We have 5 columns : sepal length, sepal width, petal length and petal width and 
a lst colums that gives the species of the row.
Let's look at the shape of the datframe iris: 
it has 150 rows and 5 columns as we already know. 
Just to be clear this dataset contains data about 150 flowers of genus iris. 
And we can see how species are distributed in this dataset usnig the 
method value_counts on the column species, 
and we see that there are in total 3 species : setosa, versicolor and virginic and that we have 50 samples of each.

In case you wonder, here are pictures of a setosa, versicolor and virginica, 
these are very nice flowers.
With seaborn we can draw a paiplot of the features of the various properties of the plants, where in blue you have setosa, in green versicolor and in red virginica.

Problem we'll try to solve is the following: 
I give you as an input the petal width of an iris plant.
And I want as an ouput an answer to the question: is the plant a virginica?
The anser could be yes (or 1 if we want a number representation) or the answer 
could be False (or equivalently 0).

To work on this we'll ad a column called 'is_virginicia' to our dataset, and the value of the column is 0 if the plant is not a virginica and 1 if it is  a virginca.
And we can call the method sample and see 5 random samples and check that we've indeed added that column.

To answer that question, it might be good to plot a histogram of the petal width
To do that we can use the method hist (for histogram) from a groupby object, and we add a legend and we get that
here on the x-axis we have the petal width, and on the y-axis, how many samples in our dataset have this petalwidth, and in green we have virginica samples while in blue we have non virginica.

What's clear from this histogram is that on the left side, so for values lower than 
1.3, the answer is False/no, it's not vifginica and we want to give it the value 0 
and when the value of the petal width is high enough freater than 1.8 the anser is yes, which we represent by the value 1.

But in between it's not so clear.
So one natural possibility here would be to draw a line to join these two partial curves.

That looks like a reasonable guess, but could also have tried to make something smoother like that: here it's again clearly 0 so false on left, and 1 so true on the right but the difference is that the transition is very smooth.

What we did so far was trying to solve the problem using our intuition, so now let's do this using Logistic regression has I promised it earlier.

For the problem we are conidering, the output of the logistic regression classifier will be a function having the petal width as input and returning as output a probability. 
And I want to stress here that a probability is a number contained between 0 and 1 and the closer it is to 0, the more the prediction is close to False, and the closer it is to 1, the closer the prediction is to be True.
Ultimaltely, the probability is used to anser the problem by saying: 
if the probability is greater then 0.5, then return True, if the probability is less than 0.5, return False.

To model probabilities the key ingredient of logistic regression is the sigmoid function.
It is the function that takes as input x and returns 1 divided by 1 + exponential of -x
This might be a little bit obscure, but let's try to plot the sigmoid function.
I define a Pythonfunction the computes the sigmoid functiona and guess how we call it: sigmoid
And we plot it

Before when we've been trying to guess dotted red curve for the probability of being virginica, we got similar curves.
Let me highlight some key properties of the curve of the sigmoid function. 
It takes values between 0 and 1 (that's we want because we want to use it to return a probability and as we said before a probability should be between 0 and 1.
It reaches the value 0.5 (one half exactly for the value x=0) and the more you go on the right, the more you tend, and the more you go on the left, the more you tend to 0.

So the idea of logistic regression is to use the sigmoid function to output the probability knowing the value of the petal width.
So let's replot the histogram of the petal widths and plot the sigmoid on the same graph.

Hum, what did we expect?  The sigmoid curve is the same as before and namely it predicts high probability (higher than 0.5) for positive values. 
So if we were using the sigmoid red curve to predict the probabilities of being virginica, we would anser True for all positive values, and since of course, the petal widths are always positive values, we would always answer True. which is bad since there are lots of plants which are not virginica.

That's where the key idea to introduce paramters enters in the game.
Namely we are going to modify the sigmoid function using two parameters
a weigth W i first give it the value 1
and a bias B i first give it the value 0
and we modify the function to compute the probability replacing x by w*x + b
If I plot again, I get the same thing, because w =1 b = 0 hence w times x + b 
equals 1*x + 0 equals x

But let's change w and make it 5 for instance. 
Can someone tell me what's going happen?

the curve is compressed around x=0
in particular the zone where the probabilities change from 0 to 1 is much smaller
this probability curve is giving which confident predicitons.

I put the value w = 1 and b= 0 again

Now what happen if I set w to 3? Has someone an idea?
it shiffts the curve to the left by 3 units
to be sure, i set w to -3 it should shift the curve to the right
yes
let's set w to 1 b =0 to get the genuine sigmoid function
and let's try to make it feet with the data we have: namely the output should be yes 
(close to 1) for values higher than 2
and close to 0 for values lower than 1.5
set w=3 to get a curve we a important slope
and now we want to shift the curve to the right, so we need a negative value for b.
try b = -1 , not enough
try b = -10 too much
try b= -5 OK
To sum up with logistic regression:
it is a model which depends on 2 parameters W called the weight and B the bias and
it ouputs a probability 
sigmoid(Wx + b)

So fine but in practice, you want your computer to do the job for you and guess himself the parameters.

For that we're gonna use first the python library scikit learn which is a library that implements many machine learning models.

We import the class Logistic regression from scikit learn
and we instatiate an instance of a logistic regression classifier and sve it to the variable model

But at that point, the model has not seen any data.
And the philosophy from machine learning is that the model should adapt to the 
data we have, and it should learn and modify its parameters so that they fit  the data
To do that, you have to call the fit method from the model, 
and give him the input data and the output in our case, the ouutput 
is 0if th plant is not virginica and 1 if the plant is virginica.

Now the model has read the data, and hopefully now the parameters fit the data

We can use the attributes coefwhich represents the weight and interept which is the bias

And it is quite close to what we had found ourselves before: we had by hand playing 
with the curve guessed w=3 b =-5

So let's try to plot the probability curve
for that we use the method predict_proba from our model
and we've got this curve,
this really agrees with what we've seen so far, it looks like the sigmiod graph 
shifted to the right

Let's plot on top of it the histogram we had
And indeed the predicted probabilities from our logistic regression model ]
which are plotted in red are very satisfying (at least very close 
to what we had suggested before)

Now an important question is how to know if your model is doing going 
on the data?
Here of course it is easy because the input has only one coordinate, 
but if there were many inputs it could be difficult to evaluate the model.

For that one introduces the accuracy of the model : 
it is the ratio between number of samples in the data which have 
been correctly classified and the total number of samples

with scikit-learn, the method score from the model returns the accuracy
and you need to give as input to this method the 
input values of the daatset (in our case the petal eidths) 
and the outuput (1 if the plant is virginica, 0 otherwise)




